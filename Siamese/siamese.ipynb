{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://cdn.pixabay.com/photo/2016/12/07/09/45/dna-1889085__340.jpg\" width=10%> <h1> Application of AI to Discover Novel Binding of Small Molecules </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### Sample Dataset for Testing Purposes\n",
    "\n",
    "##### Here we create a sample dataset for two reasons:\n",
    "- to get a better understanding of the structure of the data\n",
    "- test any sample code for validity\n",
    "\n",
    "##### Structure of sample dataset:\n",
    "1. A dataframe consisting of 50 genes and 1020 profiles [50 x 1020]\n",
    "2. Columns are a combination of drug, replicate, time, concentration, probe_location, cell type. For the purposes of this project only drug and replicate matters in terms of training. So the column name will be structured as\n",
    "\"*drug + replicate id + unique characters that represent time, concentration, probe_location and cell type*\"\n",
    "3. 20 columns consist of control genes or 'control probes'. Columns are labelled control_x where x is a number from 1 to 20\n",
    "3. Dataset consists of 25 drugs with 4 replicates and 10 combinations of time, concentration, probe_location and cell type\n",
    "\n",
    "| Feature      | Quantity | Represented By |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| Drug      | 25       | Alphabets A-Y |\n",
    "| Replicate   | 4        | Numbers 1-4 |\n",
    "| Other features   | 10        | Random String of length 3 |\n",
    "\n",
    "***R_3_xcv*** represents a profile of drug 'R', of replicate 3, with other features coresponding to 'xcv'\n",
    "\n",
    "##### Construction of Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genes = ['gene'+str(a) for a in range(50)]\n",
    "drugs = [chr(a) for a in range(65, 90)]\n",
    "replicates = [str(a) for a in range(1, 5)]\n",
    "other_features = set()\n",
    "\n",
    "while len(other_features)!=10:\n",
    "    rand_string = \"\". join([str(chr(int(random.random()*100)%26+97)) for a in range(3)])\n",
    "    other_features.add(rand_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"_\".join([a,b,c]) for a in drugs for b in replicates for c in other_features]\n",
    "# columns = [\"control_\"+str(a+1) for a in range(20)] + columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(2*np.random.rand(50, len(columns))-1, index=genes, columns=columns)\n",
    "data.columns = columns\n",
    "data.fillna(random.random(), inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_1_gps</th>\n",
       "      <th>A_1_gvy</th>\n",
       "      <th>A_1_vis</th>\n",
       "      <th>A_1_ize</th>\n",
       "      <th>A_1_opp</th>\n",
       "      <th>A_1_jbq</th>\n",
       "      <th>A_1_jkc</th>\n",
       "      <th>A_1_eit</th>\n",
       "      <th>A_1_rtv</th>\n",
       "      <th>A_1_qcj</th>\n",
       "      <th>...</th>\n",
       "      <th>Y_4_gps</th>\n",
       "      <th>Y_4_gvy</th>\n",
       "      <th>Y_4_vis</th>\n",
       "      <th>Y_4_ize</th>\n",
       "      <th>Y_4_opp</th>\n",
       "      <th>Y_4_jbq</th>\n",
       "      <th>Y_4_jkc</th>\n",
       "      <th>Y_4_eit</th>\n",
       "      <th>Y_4_rtv</th>\n",
       "      <th>Y_4_qcj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gene0</th>\n",
       "      <td>0.324634</td>\n",
       "      <td>-0.512199</td>\n",
       "      <td>-0.422898</td>\n",
       "      <td>0.322265</td>\n",
       "      <td>-0.295296</td>\n",
       "      <td>-0.563205</td>\n",
       "      <td>0.741714</td>\n",
       "      <td>-0.219386</td>\n",
       "      <td>0.263429</td>\n",
       "      <td>0.744535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271809</td>\n",
       "      <td>0.096694</td>\n",
       "      <td>0.130743</td>\n",
       "      <td>0.676475</td>\n",
       "      <td>0.053990</td>\n",
       "      <td>0.466794</td>\n",
       "      <td>-0.158463</td>\n",
       "      <td>0.668328</td>\n",
       "      <td>-0.094773</td>\n",
       "      <td>0.852537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene1</th>\n",
       "      <td>-0.261772</td>\n",
       "      <td>-0.120720</td>\n",
       "      <td>0.794160</td>\n",
       "      <td>-0.343175</td>\n",
       "      <td>0.520890</td>\n",
       "      <td>0.718406</td>\n",
       "      <td>0.892644</td>\n",
       "      <td>-0.876281</td>\n",
       "      <td>0.304717</td>\n",
       "      <td>-0.721386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.432493</td>\n",
       "      <td>-0.296607</td>\n",
       "      <td>0.739531</td>\n",
       "      <td>-0.426699</td>\n",
       "      <td>-0.595429</td>\n",
       "      <td>-0.041522</td>\n",
       "      <td>-0.463425</td>\n",
       "      <td>0.367083</td>\n",
       "      <td>-0.220199</td>\n",
       "      <td>0.098687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene2</th>\n",
       "      <td>0.055482</td>\n",
       "      <td>-0.991596</td>\n",
       "      <td>-0.683812</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.331425</td>\n",
       "      <td>0.899492</td>\n",
       "      <td>-0.769606</td>\n",
       "      <td>0.045896</td>\n",
       "      <td>0.710955</td>\n",
       "      <td>-0.489117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184052</td>\n",
       "      <td>-0.143825</td>\n",
       "      <td>0.820035</td>\n",
       "      <td>-0.866245</td>\n",
       "      <td>-0.104654</td>\n",
       "      <td>-0.585007</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>-0.158080</td>\n",
       "      <td>0.488346</td>\n",
       "      <td>-0.175934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene3</th>\n",
       "      <td>0.880006</td>\n",
       "      <td>-0.163318</td>\n",
       "      <td>0.864493</td>\n",
       "      <td>-0.922229</td>\n",
       "      <td>0.851741</td>\n",
       "      <td>-0.294169</td>\n",
       "      <td>0.432384</td>\n",
       "      <td>0.318402</td>\n",
       "      <td>-0.119600</td>\n",
       "      <td>-0.047593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103779</td>\n",
       "      <td>0.905737</td>\n",
       "      <td>-0.189061</td>\n",
       "      <td>0.832541</td>\n",
       "      <td>0.299131</td>\n",
       "      <td>0.005537</td>\n",
       "      <td>-0.317030</td>\n",
       "      <td>0.236083</td>\n",
       "      <td>-0.403237</td>\n",
       "      <td>0.324517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene4</th>\n",
       "      <td>0.510791</td>\n",
       "      <td>-0.089568</td>\n",
       "      <td>-0.228124</td>\n",
       "      <td>0.145007</td>\n",
       "      <td>0.616179</td>\n",
       "      <td>0.808682</td>\n",
       "      <td>-0.276685</td>\n",
       "      <td>0.596809</td>\n",
       "      <td>-0.399864</td>\n",
       "      <td>-0.753906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551861</td>\n",
       "      <td>-0.001323</td>\n",
       "      <td>0.091154</td>\n",
       "      <td>-0.226857</td>\n",
       "      <td>0.147661</td>\n",
       "      <td>0.007921</td>\n",
       "      <td>0.953207</td>\n",
       "      <td>-0.570335</td>\n",
       "      <td>0.408740</td>\n",
       "      <td>-0.634798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A_1_gps   A_1_gvy   A_1_vis   A_1_ize   A_1_opp   A_1_jbq   A_1_jkc  \\\n",
       "gene0  0.324634 -0.512199 -0.422898  0.322265 -0.295296 -0.563205  0.741714   \n",
       "gene1 -0.261772 -0.120720  0.794160 -0.343175  0.520890  0.718406  0.892644   \n",
       "gene2  0.055482 -0.991596 -0.683812  0.045700  0.331425  0.899492 -0.769606   \n",
       "gene3  0.880006 -0.163318  0.864493 -0.922229  0.851741 -0.294169  0.432384   \n",
       "gene4  0.510791 -0.089568 -0.228124  0.145007  0.616179  0.808682 -0.276685   \n",
       "\n",
       "        A_1_eit   A_1_rtv   A_1_qcj    ...      Y_4_gps   Y_4_gvy   Y_4_vis  \\\n",
       "gene0 -0.219386  0.263429  0.744535    ...     0.271809  0.096694  0.130743   \n",
       "gene1 -0.876281  0.304717 -0.721386    ...    -0.432493 -0.296607  0.739531   \n",
       "gene2  0.045896  0.710955 -0.489117    ...     0.184052 -0.143825  0.820035   \n",
       "gene3  0.318402 -0.119600 -0.047593    ...     0.103779  0.905737 -0.189061   \n",
       "gene4  0.596809 -0.399864 -0.753906    ...     0.551861 -0.001323  0.091154   \n",
       "\n",
       "        Y_4_ize   Y_4_opp   Y_4_jbq   Y_4_jkc   Y_4_eit   Y_4_rtv   Y_4_qcj  \n",
       "gene0  0.676475  0.053990  0.466794 -0.158463  0.668328 -0.094773  0.852537  \n",
       "gene1 -0.426699 -0.595429 -0.041522 -0.463425  0.367083 -0.220199  0.098687  \n",
       "gene2 -0.866245 -0.104654 -0.585007  0.000547 -0.158080  0.488346 -0.175934  \n",
       "gene3  0.832541  0.299131  0.005537 -0.317030  0.236083 -0.403237  0.324517  \n",
       "gene4 -0.226857  0.147661  0.007921  0.953207 -0.570335  0.408740 -0.634798  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifying Columns\n",
    "A label needs to be assigned to each class. This can be done at the biological replicate level or the perturbagen level. We create classifications for each of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perturbagen_class = [int(a/25) for a in range(1000)]\n",
    "replicate_class = [10*a+c for a in range(25) for b in range(4) for c in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene0</th>\n",
       "      <th>gene1</th>\n",
       "      <th>gene2</th>\n",
       "      <th>gene3</th>\n",
       "      <th>gene4</th>\n",
       "      <th>gene5</th>\n",
       "      <th>gene6</th>\n",
       "      <th>gene7</th>\n",
       "      <th>gene8</th>\n",
       "      <th>gene9</th>\n",
       "      <th>...</th>\n",
       "      <th>gene40</th>\n",
       "      <th>gene41</th>\n",
       "      <th>gene42</th>\n",
       "      <th>gene43</th>\n",
       "      <th>gene44</th>\n",
       "      <th>gene45</th>\n",
       "      <th>gene46</th>\n",
       "      <th>gene47</th>\n",
       "      <th>gene48</th>\n",
       "      <th>gene49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A_1_gps</th>\n",
       "      <td>0.324634</td>\n",
       "      <td>-0.261772</td>\n",
       "      <td>0.055482</td>\n",
       "      <td>0.880006</td>\n",
       "      <td>0.510791</td>\n",
       "      <td>-0.394427</td>\n",
       "      <td>0.265938</td>\n",
       "      <td>-0.270761</td>\n",
       "      <td>-0.332389</td>\n",
       "      <td>0.990229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084976</td>\n",
       "      <td>-0.485472</td>\n",
       "      <td>0.763649</td>\n",
       "      <td>0.294632</td>\n",
       "      <td>-0.839028</td>\n",
       "      <td>0.169186</td>\n",
       "      <td>0.587716</td>\n",
       "      <td>0.670123</td>\n",
       "      <td>0.507570</td>\n",
       "      <td>0.235370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_1_gvy</th>\n",
       "      <td>-0.512199</td>\n",
       "      <td>-0.120720</td>\n",
       "      <td>-0.991596</td>\n",
       "      <td>-0.163318</td>\n",
       "      <td>-0.089568</td>\n",
       "      <td>-0.330720</td>\n",
       "      <td>0.266884</td>\n",
       "      <td>0.151430</td>\n",
       "      <td>-0.963103</td>\n",
       "      <td>-0.251559</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425571</td>\n",
       "      <td>0.829806</td>\n",
       "      <td>-0.030973</td>\n",
       "      <td>-0.630850</td>\n",
       "      <td>0.590557</td>\n",
       "      <td>0.764148</td>\n",
       "      <td>0.974986</td>\n",
       "      <td>-0.073809</td>\n",
       "      <td>0.621660</td>\n",
       "      <td>0.563610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_1_vis</th>\n",
       "      <td>-0.422898</td>\n",
       "      <td>0.794160</td>\n",
       "      <td>-0.683812</td>\n",
       "      <td>0.864493</td>\n",
       "      <td>-0.228124</td>\n",
       "      <td>-0.054174</td>\n",
       "      <td>-0.444012</td>\n",
       "      <td>-0.043513</td>\n",
       "      <td>-0.696945</td>\n",
       "      <td>-0.274318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917263</td>\n",
       "      <td>0.574977</td>\n",
       "      <td>-0.024039</td>\n",
       "      <td>-0.120628</td>\n",
       "      <td>-0.432643</td>\n",
       "      <td>-0.334697</td>\n",
       "      <td>-0.604342</td>\n",
       "      <td>0.541312</td>\n",
       "      <td>0.543612</td>\n",
       "      <td>-0.751810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_1_ize</th>\n",
       "      <td>0.322265</td>\n",
       "      <td>-0.343175</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>-0.922229</td>\n",
       "      <td>0.145007</td>\n",
       "      <td>0.785617</td>\n",
       "      <td>-0.942278</td>\n",
       "      <td>0.100348</td>\n",
       "      <td>-0.217472</td>\n",
       "      <td>-0.450500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.786332</td>\n",
       "      <td>-0.660057</td>\n",
       "      <td>0.440792</td>\n",
       "      <td>-0.654501</td>\n",
       "      <td>0.639928</td>\n",
       "      <td>-0.266416</td>\n",
       "      <td>-0.491868</td>\n",
       "      <td>0.709543</td>\n",
       "      <td>-0.208610</td>\n",
       "      <td>0.670833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_1_opp</th>\n",
       "      <td>-0.295296</td>\n",
       "      <td>0.520890</td>\n",
       "      <td>0.331425</td>\n",
       "      <td>0.851741</td>\n",
       "      <td>0.616179</td>\n",
       "      <td>0.267142</td>\n",
       "      <td>-0.134957</td>\n",
       "      <td>0.925613</td>\n",
       "      <td>0.870644</td>\n",
       "      <td>-0.073282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.992558</td>\n",
       "      <td>-0.244736</td>\n",
       "      <td>0.589236</td>\n",
       "      <td>-0.009589</td>\n",
       "      <td>-0.067329</td>\n",
       "      <td>0.789043</td>\n",
       "      <td>-0.702025</td>\n",
       "      <td>0.277981</td>\n",
       "      <td>0.689783</td>\n",
       "      <td>0.017130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gene0     gene1     gene2     gene3     gene4     gene5     gene6  \\\n",
       "A_1_gps  0.324634 -0.261772  0.055482  0.880006  0.510791 -0.394427  0.265938   \n",
       "A_1_gvy -0.512199 -0.120720 -0.991596 -0.163318 -0.089568 -0.330720  0.266884   \n",
       "A_1_vis -0.422898  0.794160 -0.683812  0.864493 -0.228124 -0.054174 -0.444012   \n",
       "A_1_ize  0.322265 -0.343175  0.045700 -0.922229  0.145007  0.785617 -0.942278   \n",
       "A_1_opp -0.295296  0.520890  0.331425  0.851741  0.616179  0.267142 -0.134957   \n",
       "\n",
       "            gene7     gene8     gene9    ...       gene40    gene41    gene42  \\\n",
       "A_1_gps -0.270761 -0.332389  0.990229    ...    -0.084976 -0.485472  0.763649   \n",
       "A_1_gvy  0.151430 -0.963103 -0.251559    ...    -0.425571  0.829806 -0.030973   \n",
       "A_1_vis -0.043513 -0.696945 -0.274318    ...     0.917263  0.574977 -0.024039   \n",
       "A_1_ize  0.100348 -0.217472 -0.450500    ...    -0.786332 -0.660057  0.440792   \n",
       "A_1_opp  0.925613  0.870644 -0.073282    ...    -0.992558 -0.244736  0.589236   \n",
       "\n",
       "           gene43    gene44    gene45    gene46    gene47    gene48    gene49  \n",
       "A_1_gps  0.294632 -0.839028  0.169186  0.587716  0.670123  0.507570  0.235370  \n",
       "A_1_gvy -0.630850  0.590557  0.764148  0.974986 -0.073809  0.621660  0.563610  \n",
       "A_1_vis -0.120628 -0.432643 -0.334697 -0.604342  0.541312  0.543612 -0.751810  \n",
       "A_1_ize -0.654501  0.639928 -0.266416 -0.491868  0.709543 -0.208610  0.670833  \n",
       "A_1_opp -0.009589 -0.067329  0.789043 -0.702025  0.277981  0.689783  0.017130  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transpose data\n",
    "workingdata = data.transpose()\n",
    "workingdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(workingdata, perturbagen_class, test_size=0.5)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computation - Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 50\n",
    "batch_size = 16\n",
    "epochs = 40\n",
    "\n",
    "def create_network(n_dense=6,\n",
    "                   dense_units=16,\n",
    "                   activation='selu',\n",
    "                   dropout=AlphaDropout,\n",
    "                   dropout_rate=0.1,\n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   optimizer='adam',\n",
    "                   num_classes=1,\n",
    "                   max_words=max_words):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_units, input_shape=(max_words,),\n",
    "                    kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(dropout(dropout_rate))\n",
    "\n",
    "    for i in range(n_dense - 1):\n",
    "        model.add(Dense(dense_units, kernel_initializer=kernel_initializer))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(dropout(dropout_rate))\n",
    "\n",
    "    #model.add(Dense(num_classes))\n",
    "    #model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = {\n",
    "    'n_dense': 10,\n",
    "    'dense_units': 16,\n",
    "    'activation': 'selu',\n",
    "    'dropout': AlphaDropout,\n",
    "    'dropout_rate': 0.1,\n",
    "    'kernel_initializer': 'lecun_normal',\n",
    "    'optimizer': 'sgd',\n",
    "    'num_classes':40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_network(**network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ManDist(Layer):\n",
    "    \n",
    "    # initialize the layer, No need to include inputs parameter!\n",
    "    def __init__(self, **kwargs):\n",
    "        self.result = None\n",
    "        super(ManDist, self).__init__(**kwargs)\n",
    "\n",
    "    # input_shape will automatic collect input shapes to build layer\n",
    "    def build(self, input_shape):\n",
    "        super(ManDist, self).build(input_shape)\n",
    "\n",
    "    # This is where the layer's logic lives.\n",
    "    def call(self, x, **kwargs):\n",
    "        self.result = K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True)\n",
    "        return self.result\n",
    "\n",
    "    # return output shape\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return K.int_shape(self.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_input = Input(shape=(max_words,))\n",
    "right_input = Input(shape=(max_words,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model variables\n",
    "shared_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 16)           3264        input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "man_dist_1 (ManDist)            (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,264\n",
      "Trainable params: 3,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "alpha_dropout_1 (AlphaDropou (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "alpha_dropout_2 (AlphaDropou (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "alpha_dropout_3 (AlphaDropou (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "alpha_dropout_4 (AlphaDropou (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "alpha_dropout_5 (AlphaDropou (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "alpha_dropout_6 (AlphaDropou (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "alpha_dropout_7 (AlphaDropou (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "alpha_dropout_8 (AlphaDropou (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "alpha_dropout_9 (AlphaDropou (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "alpha_dropout_10 (AlphaDropo (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 3,264\n",
      "Trainable params: 3,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'- embedding layer is required\n",
    "#Node error -> from keras not from tf.python.keras\n",
    "#Input 'b' of 'MatMul' Op has type float32 that does not match type int32 of argument 'a'. ->\n",
    "malstm_distance = ManDist()([shared_model(left_input), shared_model(right_input)])\n",
    "model = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()\n",
    "shared_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.asarray(data.transpose().iloc[0:5,:])\n",
    "b = np.asarray(data.transpose().iloc[6:11,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.19057459,  1.7253517 , -1.6672118 , -0.1841178 ,  0.57209563,\n",
       "         -1.1599962 ,  0.5523666 ,  0.5079637 ,  0.24208523, -1.1146371 ,\n",
       "         -1.111214  ,  0.8422288 ,  2.0185504 , -0.52523667,  0.45260412,\n",
       "          0.33307612],\n",
       "        [-0.39516768, -0.29591113, -1.5629207 ,  1.0116124 ,  0.17067975,\n",
       "          0.30304193,  1.8597828 , -0.69617605,  0.8184404 , -0.5511421 ,\n",
       "         -0.8471207 ,  0.44013354,  0.47050068, -0.13658464, -1.3789027 ,\n",
       "          1.1954532 ],\n",
       "        [ 0.44339842,  1.099096  , -1.5248781 , -0.8852858 ,  0.839443  ,\n",
       "         -1.4595574 ,  0.32434022,  1.4775376 , -1.0955718 , -0.37081409,\n",
       "         -1.2843187 ,  1.7918526 , -0.5683673 , -1.0145888 ,  0.21341445,\n",
       "         -0.78439206],\n",
       "        [-0.52113116, -0.6008608 ,  1.3506528 ,  0.2787183 , -1.4213215 ,\n",
       "          1.9024509 ,  0.00892367, -1.4003485 ,  0.92487144, -0.45669106,\n",
       "          1.0628775 , -1.3624649 ,  0.61842984,  0.926004  , -0.45385814,\n",
       "          1.1992599 ],\n",
       "        [-0.5786603 ,  0.07188167, -1.5112418 ,  0.14983733, -0.5013617 ,\n",
       "          0.49192253,  1.5762625 , -1.0974182 ,  1.16274   , -0.8469576 ,\n",
       "         -0.64428467,  0.16862479,  1.1209757 ,  0.04125013, -0.89475626,\n",
       "          1.1462153 ]], dtype=float32),\n",
       " array([[-0.9234279 , -0.17551939, -1.5756048 ,  1.2789413 ,  0.2910808 ,\n",
       "          0.79200834,  1.4569635 , -1.0875822 ,  0.1841475 , -0.16596948,\n",
       "         -0.5837196 ,  0.31212524,  1.2724271 , -0.02394294, -1.4347425 ,\n",
       "          1.644362  ],\n",
       "        [-0.70071787,  0.4812163 , -1.5138249 ,  0.7173757 , -0.76914173,\n",
       "          0.85516423,  1.5072391 , -1.1786807 ,  1.524909  , -0.8127037 ,\n",
       "         -0.48476705, -0.60339135,  1.7025353 ,  0.35083437, -0.81651604,\n",
       "          1.7285175 ],\n",
       "        [-0.40187976, -1.1970252 ,  1.1993293 ,  0.7851771 , -0.78321517,\n",
       "          1.0963804 ,  0.36474133, -1.183065  ,  0.08949649,  0.10320535,\n",
       "          0.6426037 , -1.0554821 , -0.21483809,  0.3605933 , -1.0419867 ,\n",
       "          0.63765585],\n",
       "        [-1.1941949 , -0.9949507 ,  1.4958904 ,  1.248396  , -0.8446916 ,\n",
       "          1.7145752 , -0.1616698 , -1.3022183 , -0.04538776,  0.7608135 ,\n",
       "          1.0247623 , -1.1667564 ,  0.05879019,  0.6371444 , -0.8507742 ,\n",
       "          0.60094666],\n",
       "        [-0.1911895 ,  2.1288178 , -1.4749514 , -0.97989225,  0.6305361 ,\n",
       "         -1.394404  , -0.6285366 ,  1.2815014 , -0.665473  , -0.66692734,\n",
       "         -1.0140779 ,  1.1657441 ,  1.0110542 , -0.69816977,  1.3675956 ,\n",
       "         -0.9357735 ]], dtype=float32),\n",
       " array([[15.431806 ],\n",
       "        [ 8.941815 ],\n",
       "        [25.255173 ],\n",
       "        [ 7.4806147],\n",
       "        [19.781208 ]], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# with a Sequential model\n",
    "get_3rd_layer_output = K.function([model.layers[0].input,model.layers[1].input]\n",
    "                                  ,[model.layers[2].get_output_at(1),model.layers[2].get_output_at(2),model.layers[3].output])\n",
    "\n",
    "layer_output = get_3rd_layer_output([a,b])\n",
    "layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.431805327534676"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = layer_output[0][0]\n",
    "\n",
    "two = layer_output[1][0]\n",
    "\n",
    "result = sum(abs(one - two))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 154.5413 - acc: 0.0220\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 0s 178us/step - loss: 154.9352 - acc: 0.0140\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 0s 171us/step - loss: 151.4432 - acc: 0.0280\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 0s 172us/step - loss: 152.3120 - acc: 0.0280\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 0s 184us/step - loss: 147.2933 - acc: 0.0220\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 0s 180us/step - loss: 154.3417 - acc: 0.0200\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 0s 185us/step - loss: 152.3505 - acc: 0.0220\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 0s 183us/step - loss: 151.8972 - acc: 0.0180\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 0s 181us/step - loss: 152.4634 - acc: 0.0220\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 0s 187us/step - loss: 163.9644 - acc: 0.0360\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 0s 221us/step - loss: 149.6900 - acc: 0.0380\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 0s 215us/step - loss: 144.6647 - acc: 0.0220\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 0s 207us/step - loss: 152.7282 - acc: 0.0260\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 0s 178us/step - loss: 152.1551 - acc: 0.0200\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 0s 188us/step - loss: 155.4072 - acc: 0.0240\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 0s 188us/step - loss: 152.7927 - acc: 0.0080\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 0s 181us/step - loss: 155.0687 - acc: 0.0360\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 0s 176us/step - loss: 151.7291 - acc: 0.0140\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 0s 197us/step - loss: 147.3643 - acc: 0.0220\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 0s 171us/step - loss: 153.3584 - acc: 0.0320\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 0s 204us/step - loss: 148.9180 - acc: 0.0320\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 0s 189us/step - loss: 151.1166 - acc: 0.0180\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 0s 189us/step - loss: 142.5392 - acc: 0.0160\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 0s 179us/step - loss: 145.3579 - acc: 0.0140\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 0s 191us/step - loss: 141.2733 - acc: 0.0300\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 0s 180us/step - loss: 154.9920 - acc: 0.0200\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 0s 181us/step - loss: 146.4294 - acc: 0.0160\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 0s 185us/step - loss: 153.3572 - acc: 0.0160\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 0s 186us/step - loss: 138.2391 - acc: 0.0180\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 0s 184us/step - loss: 156.4441 - acc: 0.0180\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 0s 187us/step - loss: 149.9218 - acc: 0.0280\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 0s 186us/step - loss: 149.2499 - acc: 0.0280\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 0s 182us/step - loss: 142.6096 - acc: 0.0260\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 0s 184us/step - loss: 147.2215 - acc: 0.0280\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 0s 183us/step - loss: 154.1151 - acc: 0.0240\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 0s 182us/step - loss: 145.8684 - acc: 0.0260\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 0s 184us/step - loss: 151.3580 - acc: 0.0180\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 0s 177us/step - loss: 144.3251 - acc: 0.0220\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 0s 178us/step - loss: 148.7935 - acc: 0.0220\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 0s 183us/step - loss: 149.1690 - acc: 0.0200\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 0s 186us/step - loss: 144.4119 - acc: 0.0320\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 0s 189us/step - loss: 152.7744 - acc: 0.0240\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 0s 188us/step - loss: 153.8181 - acc: 0.0160\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 0s 183us/step - loss: 149.8381 - acc: 0.0240\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 0s 180us/step - loss: 148.3202 - acc: 0.0180\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 0s 183us/step - loss: 145.3694 - acc: 0.0220\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 0s 187us/step - loss: 152.6152 - acc: 0.0320\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 0s 181us/step - loss: 147.2332 - acc: 0.0380\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 0s 181us/step - loss: 139.0876 - acc: 0.0160\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 0s 186us/step - loss: 141.7084 - acc: 0.0180\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 0s 180us/step - loss: 144.6778 - acc: 0.0120\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 0s 185us/step - loss: 144.7825 - acc: 0.0280\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 0s 180us/step - loss: 155.8700 - acc: 0.0260\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 0s 177us/step - loss: 146.2461 - acc: 0.0360\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 0s 179us/step - loss: 145.3727 - acc: 0.0260\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 0s 189us/step - loss: 157.8106 - acc: 0.0200\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 0s 181us/step - loss: 145.8178 - acc: 0.0140\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 0s 181us/step - loss: 150.9979 - acc: 0.0260\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 0s 185us/step - loss: 147.2274 - acc: 0.0280\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 0s 182us/step - loss: 148.4974 - acc: 0.0280\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 0s 183us/step - loss: 147.6467 - acc: 0.0260\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 0s 184us/step - loss: 137.1056 - acc: 0.0180\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 0s 184us/step - loss: 144.4432 - acc: 0.0260\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 0s 183us/step - loss: 150.7597 - acc: 0.0100\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 0s 186us/step - loss: 142.0738 - acc: 0.0300\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 0s 180us/step - loss: 146.7632 - acc: 0.0220\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 0s 185us/step - loss: 146.1788 - acc: 0.0260\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 0s 183us/step - loss: 146.9797 - acc: 0.0280\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 0s 185us/step - loss: 144.9973 - acc: 0.0300\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 0s 180us/step - loss: 142.8491 - acc: 0.0300\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 0s 179us/step - loss: 139.2787 - acc: 0.0160\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 0s 192us/step - loss: 151.5498 - acc: 0.0260\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 0s 186us/step - loss: 146.9423 - acc: 0.0300\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 0s 180us/step - loss: 150.2802 - acc: 0.0220\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 0s 313us/step - loss: 148.0623 - acc: 0.0300\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 0s 247us/step - loss: 143.1297 - acc: 0.0200\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 0s 184us/step - loss: 153.6449 - acc: 0.0200\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 0s 182us/step - loss: 149.0304 - acc: 0.0340\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 0s 191us/step - loss: 147.3321 - acc: 0.0280\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 0s 192us/step - loss: 143.4506 - acc: 0.0260\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 0s 189us/step - loss: 136.0978 - acc: 0.0280\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 185us/step - loss: 139.6141 - acc: 0.0260\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 0s 187us/step - loss: 140.1346 - acc: 0.0240\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 0s 170us/step - loss: 140.0917 - acc: 0.0220\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 0s 175us/step - loss: 143.7316 - acc: 0.0300\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 0s 188us/step - loss: 143.6597 - acc: 0.0260\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 0s 180us/step - loss: 136.6110 - acc: 0.0360\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 0s 186us/step - loss: 141.4366 - acc: 0.0260\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 0s 182us/step - loss: 143.3236 - acc: 0.0200\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 0s 179us/step - loss: 141.9958 - acc: 0.0340\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 0s 165us/step - loss: 145.3823 - acc: 0.0300\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 0s 164us/step - loss: 143.4597 - acc: 0.0280\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 0s 178us/step - loss: 137.2283 - acc: 0.0200\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 0s 179us/step - loss: 140.9396 - acc: 0.0300\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 0s 178us/step - loss: 138.5339 - acc: 0.0280\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 0s 173us/step - loss: 142.3574 - acc: 0.0300\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 0s 172us/step - loss: 134.2056 - acc: 0.0140\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 0s 167us/step - loss: 141.9414 - acc: 0.0220\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 0s 172us/step - loss: 137.3839 - acc: 0.0160\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 0s 172us/step - loss: 138.8810 - acc: 0.0340\n"
     ]
    }
   ],
   "source": [
    "#ValueError: Error when checking target: expected man_dist_1 to have shape (1,) but got array with shape (46,)\n",
    "#==> need to convert code to suit multi-class\n",
    "\n",
    "malstm_trained = model.fit([X_train,X_test], y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 704us/step\n",
      "[[ 1.7313926]\n",
      " [14.990277 ]\n",
      " [35.39305  ]\n",
      " [38.89383  ]\n",
      " [12.070194 ]]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict([X_test,X_train],verbose=1)\n",
    "print(prediction[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 747us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[275.4162053222656, 0.036]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate([X_test,X_train],y_train,verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.33087402e-02,  1.50969541e-02,  1.83586195e-01,\n",
       "        -1.21911585e-01,  2.19751149e-01, -1.70956686e-01,\n",
       "        -1.65725246e-01,  2.01449186e-01, -6.53685778e-02,\n",
       "        -3.84455286e-02,  1.99463703e-02, -3.30686532e-02,\n",
       "         2.02107176e-01,  1.59298882e-01,  1.53938800e-01,\n",
       "         9.80899855e-02],\n",
       "       [ 2.41964743e-01, -5.12437522e-02,  5.88559918e-02,\n",
       "         2.90473215e-02, -9.78905335e-02, -1.00420691e-01,\n",
       "         4.85755205e-02,  4.00516056e-02, -2.23506838e-01,\n",
       "        -6.63971454e-02,  1.73637360e-01,  9.34533179e-02,\n",
       "         1.32114105e-02, -9.23417136e-02,  6.21325634e-02,\n",
       "         1.78486127e-02],\n",
       "       [ 4.36170429e-01,  8.08338821e-02, -1.50025627e-02,\n",
       "         2.08037905e-03,  1.47101626e-01,  1.40257806e-01,\n",
       "         2.66554505e-01,  2.35853195e-01,  1.80527627e-01,\n",
       "         2.45074153e-01, -4.17984836e-02,  5.09905666e-02,\n",
       "         1.54581219e-01, -2.36850426e-01,  2.13808715e-01,\n",
       "         8.51523578e-02],\n",
       "       [ 2.00655133e-01,  1.07609473e-01,  1.68777928e-01,\n",
       "         8.83683786e-02,  1.03104129e-01, -3.09959680e-01,\n",
       "         9.40042213e-02,  2.81151801e-01,  3.58326703e-01,\n",
       "         1.41077563e-01,  4.85045686e-02,  2.40130126e-01,\n",
       "         5.51201105e-02,  2.13376358e-01,  2.80975133e-01,\n",
       "        -1.47857055e-01],\n",
       "       [-3.16786505e-02,  6.85122907e-02, -2.68205106e-01,\n",
       "         6.43509775e-02, -2.19963416e-01,  1.35806099e-01,\n",
       "        -5.85888922e-02, -2.40091667e-01,  6.98669329e-02,\n",
       "         5.21705896e-02, -4.23219316e-02,  9.92278606e-02,\n",
       "         1.12343840e-01,  1.56989247e-01,  1.49014220e-01,\n",
       "         8.01537260e-02],\n",
       "       [ 2.79950172e-01,  1.20494150e-01, -3.29832517e-04,\n",
       "        -5.40033989e-02,  8.12139213e-02, -5.88523410e-02,\n",
       "        -1.41250603e-02,  3.68638873e-01,  7.67558813e-02,\n",
       "        -6.74758181e-02, -2.19350129e-01, -1.28199145e-01,\n",
       "        -7.56140277e-02, -1.72365844e-01, -2.67578941e-03,\n",
       "        -1.76715046e-01],\n",
       "       [-5.02154492e-02, -1.25715032e-01, -6.54100031e-02,\n",
       "         2.50425525e-02, -9.94924903e-02, -1.21171132e-01,\n",
       "        -6.62161857e-02,  1.45905793e-01,  3.46961687e-03,\n",
       "         1.75794214e-01, -1.08645752e-01,  2.13562191e-01,\n",
       "         1.13643788e-01,  1.29732057e-01, -7.90920630e-02,\n",
       "        -1.33720607e-01],\n",
       "       [-2.42296949e-01, -4.19860408e-02, -2.08612233e-01,\n",
       "         1.07909236e-02, -7.88138211e-02, -1.88619271e-01,\n",
       "        -2.11308107e-01, -2.65554368e-01, -2.64274478e-01,\n",
       "        -1.34243622e-01, -1.07703201e-01,  1.50002733e-01,\n",
       "        -8.10264144e-03,  1.49163380e-01, -7.94898048e-02,\n",
       "         1.32723600e-01],\n",
       "       [ 2.17406571e-01, -7.87468720e-03, -1.74982458e-01,\n",
       "        -2.10880607e-01,  1.24865949e-01, -8.87744594e-03,\n",
       "         7.32455030e-02,  4.34105918e-02,  3.55601050e-02,\n",
       "         5.80903590e-02,  5.21730036e-02, -6.01754617e-03,\n",
       "         1.12445638e-01,  1.43879175e-01,  1.19338237e-01,\n",
       "        -2.60583848e-01],\n",
       "       [ 1.18656904e-01,  1.19860567e-01,  1.46557361e-01,\n",
       "        -1.85188875e-01,  1.06922202e-01,  7.04441369e-02,\n",
       "         1.72831714e-01, -1.02155812e-01,  1.24429859e-01,\n",
       "         3.59558046e-01, -4.55927104e-02, -4.12666708e-01,\n",
       "         2.76886076e-01, -1.18089467e-01,  4.02092896e-02,\n",
       "         1.53490573e-01],\n",
       "       [-1.59628555e-01, -1.05505055e-02, -1.20777287e-01,\n",
       "        -2.13440403e-01,  1.39115117e-02,  1.77370645e-02,\n",
       "         4.07415293e-02, -2.22787157e-01, -2.43778557e-01,\n",
       "        -2.33895168e-01,  4.83332910e-02, -1.55547664e-01,\n",
       "        -1.38298482e-01, -1.02561094e-01, -4.36487645e-02,\n",
       "         1.72881901e-01],\n",
       "       [ 1.60920158e-01,  1.40921593e-01,  1.96365029e-01,\n",
       "        -2.12559342e-01, -4.18177852e-03, -1.27936542e-01,\n",
       "        -1.13184705e-01,  1.60297304e-01, -2.61205677e-02,\n",
       "        -3.68842743e-02, -1.32960841e-01, -8.21892545e-02,\n",
       "        -1.65593237e-01,  1.22942016e-01,  1.69540256e-01,\n",
       "        -6.75332174e-02],\n",
       "       [ 4.89657000e-02, -1.17469735e-01,  2.50963885e-02,\n",
       "         2.36440059e-02,  1.05577528e-01, -2.86119208e-02,\n",
       "         1.98419705e-01, -2.07065552e-01, -1.26321092e-01,\n",
       "         9.85758826e-02,  1.73026491e-02, -2.09310532e-01,\n",
       "         1.03264660e-01,  1.43717110e-01, -1.47781581e-01,\n",
       "        -1.29765272e-01],\n",
       "       [-1.45629317e-01,  1.19113609e-01,  1.72337860e-01,\n",
       "         1.81595951e-01, -1.91587821e-01,  1.47827789e-01,\n",
       "        -1.57977968e-01, -4.30161171e-02, -5.36009204e-03,\n",
       "        -1.17366493e-01, -3.35391760e-01, -2.26878926e-01,\n",
       "        -1.32775471e-01, -2.29712233e-01, -1.46228224e-01,\n",
       "        -1.78293847e-02],\n",
       "       [ 6.44352511e-02, -2.68730640e-01, -1.20623976e-01,\n",
       "        -6.35649683e-03, -3.11230421e-01,  2.73027923e-02,\n",
       "         2.79897690e-01,  2.45354027e-01, -1.00414701e-01,\n",
       "         1.46720275e-01,  8.46072882e-02, -8.15180168e-02,\n",
       "         2.12711290e-01,  2.57146329e-01,  8.19913521e-02,\n",
       "         7.67596392e-03],\n",
       "       [-3.33155453e-01, -8.14397559e-02, -3.15397531e-01,\n",
       "         2.29783520e-01, -1.37302190e-01, -3.48616615e-02,\n",
       "        -9.39380229e-02, -2.17386290e-01, -3.82294893e-01,\n",
       "        -2.15508863e-01,  2.12674662e-01,  8.82602707e-02,\n",
       "        -2.89486229e-01,  9.34422314e-02,  5.48900887e-02,\n",
       "         2.60769218e-01],\n",
       "       [-1.20552361e-01, -1.42889127e-01,  1.80014949e-02,\n",
       "         9.84475482e-03, -4.22264598e-02, -2.05313936e-01,\n",
       "        -1.21894121e-01,  3.30605432e-02, -6.71203882e-02,\n",
       "        -9.83613059e-02,  2.98701823e-01, -2.03937903e-01,\n",
       "         1.49538279e-01, -1.68183908e-01,  1.95406541e-01,\n",
       "        -2.93483119e-02],\n",
       "       [ 2.17404097e-01,  2.60775596e-01,  1.66117743e-01,\n",
       "         1.23073116e-01,  2.88599022e-02, -4.75393422e-02,\n",
       "         6.48504309e-03, -9.13016200e-02, -1.97471455e-01,\n",
       "        -3.02466098e-02, -1.33038148e-01,  6.49377587e-04,\n",
       "         1.99944973e-01, -1.41936392e-01,  1.43245280e-01,\n",
       "        -3.03671926e-01],\n",
       "       [ 1.65796131e-01, -8.06578249e-02,  1.68989956e-01,\n",
       "         3.10813114e-02,  3.11706215e-02, -2.14144826e-01,\n",
       "         7.36946985e-02,  9.91571695e-02,  9.47221741e-02,\n",
       "         3.22497897e-02, -1.05921617e-02,  2.18913630e-01,\n",
       "         1.75294913e-02,  7.55855218e-02,  4.26321011e-03,\n",
       "         6.79048300e-02],\n",
       "       [-1.18700489e-01, -2.86816776e-01,  1.32413402e-01,\n",
       "        -8.28968063e-02, -1.20581977e-01,  6.70629367e-02,\n",
       "         1.49279177e-01, -8.58600587e-02, -3.04940313e-01,\n",
       "         8.24044794e-02,  1.23595931e-01,  3.50090973e-02,\n",
       "        -3.78129520e-02,  2.42583930e-01, -1.26664117e-01,\n",
       "        -5.03390236e-03],\n",
       "       [ 2.88741678e-01,  1.54378772e-01,  2.31984690e-01,\n",
       "        -3.01483572e-02,  1.10563584e-01,  4.99748699e-02,\n",
       "         1.46193787e-01,  6.86305836e-02,  1.17907420e-01,\n",
       "         1.79023623e-01, -8.79148245e-02,  8.29559937e-02,\n",
       "         2.78131932e-01,  5.78230247e-02,  2.40406707e-01,\n",
       "         3.29750553e-02],\n",
       "       [-4.45258357e-02,  4.78319451e-02, -7.01071322e-02,\n",
       "         6.97898567e-02,  1.45722955e-01,  4.32431810e-02,\n",
       "         8.82802159e-02, -3.31323892e-02,  2.34503578e-02,\n",
       "         8.47728401e-02,  6.95728348e-04,  1.79211289e-01,\n",
       "         4.71051522e-02, -1.48468465e-01,  2.89168209e-01,\n",
       "        -4.60067764e-02],\n",
       "       [ 1.73430026e-01,  1.52891278e-01, -2.55388677e-01,\n",
       "        -1.98532343e-01,  7.44395033e-02,  1.62190571e-01,\n",
       "         3.98697704e-02, -4.43799943e-02,  2.94212382e-02,\n",
       "        -7.33271055e-03,  9.85372141e-02, -1.40980229e-01,\n",
       "         1.50105804e-01, -5.63832559e-02,  2.93215346e-02,\n",
       "         9.69851390e-02],\n",
       "       [-2.45951489e-01,  5.50105572e-02,  1.63448244e-01,\n",
       "         8.91937464e-02,  6.45586476e-02,  9.69672389e-03,\n",
       "         9.56909284e-02,  1.80662662e-01,  2.24367157e-02,\n",
       "         1.36913702e-01,  1.40841022e-01, -2.14065909e-01,\n",
       "        -9.05481353e-02,  2.46845521e-02, -2.65307836e-02,\n",
       "         2.25899965e-02],\n",
       "       [ 1.80504918e-01, -3.25280249e-01, -2.62404531e-01,\n",
       "        -8.41827393e-02,  1.39649466e-01, -6.73073605e-02,\n",
       "        -1.02578811e-01,  2.73506045e-02,  1.43896639e-01,\n",
       "         1.65908381e-01,  6.27493858e-02, -4.43449281e-02,\n",
       "        -1.99198335e-01, -2.10354224e-01, -9.63743925e-02,\n",
       "         1.54987797e-01],\n",
       "       [ 7.16365054e-02,  2.19873801e-01, -8.04262161e-02,\n",
       "        -3.50683443e-02, -2.13392675e-01, -8.49754140e-02,\n",
       "         3.00261155e-02, -1.98787570e-01, -5.63246645e-02,\n",
       "         1.99273676e-01,  3.66501277e-03, -3.46615374e-01,\n",
       "         2.91493654e-01,  3.70311812e-02,  2.30446115e-01,\n",
       "         2.04766653e-02],\n",
       "       [ 9.47081745e-02,  5.91126271e-02, -1.33618906e-01,\n",
       "         9.17167589e-02,  2.96069980e-01, -1.00970566e-01,\n",
       "         2.62769192e-01, -6.80092573e-02,  1.64094716e-01,\n",
       "        -4.90445569e-02, -1.20805211e-01, -4.31329869e-02,\n",
       "        -5.95319979e-02, -1.27994969e-01,  1.21927284e-01,\n",
       "         5.49257249e-02],\n",
       "       [ 1.58167019e-01,  1.06557503e-01,  2.02470180e-02,\n",
       "         1.14842139e-01, -2.21295148e-01,  1.88773528e-01,\n",
       "        -1.96380839e-01, -3.53469700e-02, -2.79562891e-01,\n",
       "         9.82870981e-02, -2.76946753e-01, -6.00186847e-02,\n",
       "        -6.46959245e-02, -2.21988000e-03,  3.37992166e-03,\n",
       "        -1.70125157e-01],\n",
       "       [ 1.25180125e-01, -2.05426455e-01,  1.87426154e-02,\n",
       "        -2.40090806e-02, -6.19137734e-02, -9.91525594e-03,\n",
       "        -5.94569929e-02, -5.75360470e-03,  8.07722658e-02,\n",
       "        -1.95759863e-01, -1.66666225e-01,  2.80439794e-01,\n",
       "        -1.11031137e-01, -4.64242660e-02, -1.30742818e-01,\n",
       "        -6.30907193e-02],\n",
       "       [-1.61786303e-01,  7.72940367e-02, -8.76657590e-02,\n",
       "        -7.92702101e-03,  1.03187844e-01, -4.90881465e-02,\n",
       "         6.02937415e-02, -1.02642678e-01,  1.73889369e-01,\n",
       "         2.01726764e-01, -4.03967984e-02, -2.48658974e-02,\n",
       "        -1.43534109e-01,  1.49350762e-01,  6.99519962e-02,\n",
       "        -1.84067354e-01],\n",
       "       [-7.06170946e-02, -2.66022056e-01, -3.64793330e-01,\n",
       "        -4.49064523e-02, -2.15400279e-01, -1.13522448e-01,\n",
       "        -1.96604967e-01, -1.44149721e-01, -1.88164145e-01,\n",
       "        -2.66236395e-01, -1.80350199e-01,  1.63066283e-01,\n",
       "        -1.05784960e-01,  1.36888653e-01, -2.63353229e-01,\n",
       "         2.60151207e-01],\n",
       "       [-1.09839469e-01, -1.07388444e-01, -2.44274110e-01,\n",
       "        -5.97762037e-03, -1.77379414e-01,  1.45320788e-01,\n",
       "         7.76176453e-02, -2.66740322e-02, -1.25912100e-01,\n",
       "        -1.54292628e-01,  1.49948314e-01,  2.61468112e-01,\n",
       "         5.00402451e-02,  1.33398667e-01, -2.61265725e-01,\n",
       "         7.20061585e-02],\n",
       "       [-1.09628573e-01, -9.43119377e-02, -1.25232518e-01,\n",
       "        -8.81486982e-02, -1.19907089e-01,  1.20129123e-01,\n",
       "        -2.16015697e-01, -1.18004784e-01, -2.37686306e-01,\n",
       "        -1.65447831e-01,  1.56138122e-01,  1.07753342e-02,\n",
       "         5.09518422e-02,  4.46747541e-02,  6.80208206e-02,\n",
       "         9.02993977e-02],\n",
       "       [-3.26075047e-01,  1.23316832e-01,  4.33920585e-02,\n",
       "         1.53560013e-01,  1.36618271e-01, -1.09667793e-01,\n",
       "        -1.18082136e-01, -2.34042138e-01,  9.39184874e-02,\n",
       "        -1.23741560e-01,  3.17936242e-02,  6.61930144e-02,\n",
       "        -5.29125221e-02, -2.04970613e-01, -3.35423142e-01,\n",
       "         1.28274232e-01],\n",
       "       [ 3.87614042e-01,  1.76988766e-01,  1.76695868e-01,\n",
       "         9.14998204e-02,  2.57511765e-01,  1.08152643e-01,\n",
       "        -2.28205636e-01,  3.17819804e-01,  1.93992853e-02,\n",
       "        -9.22318026e-02,  2.26458415e-01, -8.04207996e-02,\n",
       "         3.74174982e-01,  7.53788054e-02,  3.05804580e-01,\n",
       "        -1.65781856e-01],\n",
       "       [-2.01444387e-01, -7.90125057e-02,  2.10315198e-01,\n",
       "         3.06275040e-02,  2.79561095e-02,  1.08850546e-01,\n",
       "         7.07764775e-02, -6.92040622e-02, -7.59920776e-02,\n",
       "         4.95472960e-02, -2.51189113e-01, -2.63379186e-01,\n",
       "         9.78907011e-03, -8.90248492e-02,  1.31558105e-01,\n",
       "        -2.32662544e-01],\n",
       "       [ 2.65961796e-01,  2.22056180e-01,  1.10424152e-02,\n",
       "         1.51782304e-01, -6.37731254e-02, -1.09876446e-01,\n",
       "        -5.40174842e-02, -1.61158919e-01, -3.40336151e-02,\n",
       "         1.20909981e-01,  1.28266394e-01, -2.45604262e-01,\n",
       "         1.95744246e-01, -2.24998116e-01,  1.86592877e-01,\n",
       "         9.00598913e-02],\n",
       "       [ 1.58153489e-01,  2.11744338e-01, -2.59767950e-01,\n",
       "         1.04474254e-01,  1.41353682e-01,  6.02853997e-03,\n",
       "         3.43640260e-02, -7.83970132e-02, -4.79409099e-02,\n",
       "        -1.34547755e-01,  1.01950541e-02, -1.34942159e-01,\n",
       "        -2.56513029e-01,  2.22742960e-01,  5.05338311e-02,\n",
       "         4.89901602e-02],\n",
       "       [ 2.97113150e-01, -1.45789078e-02,  3.27388734e-01,\n",
       "         1.96517780e-01, -8.68347436e-02,  1.18378289e-01,\n",
       "         2.31350198e-01, -1.68734114e-03, -6.04258478e-02,\n",
       "         4.86544007e-03, -7.00224862e-02, -7.28682652e-02,\n",
       "         1.53391927e-01, -9.33570415e-02,  1.16627671e-01,\n",
       "        -2.91168876e-02],\n",
       "       [-1.08639263e-01, -1.15854569e-01,  7.08456561e-02,\n",
       "         7.36374259e-02, -3.66727591e-01,  5.52059971e-02,\n",
       "         1.12983391e-01,  2.06383392e-01, -2.56449640e-01,\n",
       "         8.05634726e-03,  6.28723130e-02, -1.95882007e-01,\n",
       "        -3.04905660e-02, -2.64049143e-01,  2.09610201e-02,\n",
       "         1.38530031e-01],\n",
       "       [-5.38047664e-02, -2.22663254e-01,  4.02569398e-02,\n",
       "        -5.48982769e-02,  2.20610127e-01,  6.05579130e-02,\n",
       "         1.29765585e-01,  1.19888552e-01, -3.47205102e-02,\n",
       "         2.78816044e-01,  1.34518221e-02,  4.30213809e-02,\n",
       "        -1.49075044e-02,  1.30371243e-01,  4.53442074e-02,\n",
       "         5.42358495e-02],\n",
       "       [ 2.00911760e-01,  9.48738828e-02, -2.61214256e-01,\n",
       "        -5.73387183e-02,  2.00436950e-01, -2.68693656e-01,\n",
       "        -7.42957974e-03,  1.89478040e-01,  2.30671033e-01,\n",
       "         2.90059924e-01,  3.62455137e-02, -1.60003509e-02,\n",
       "         1.55818775e-01, -7.29285777e-02,  2.27226615e-01,\n",
       "        -2.31330544e-02],\n",
       "       [ 1.44071519e-01,  1.91211239e-01,  2.27517009e-01,\n",
       "         1.99276611e-01,  1.18273720e-01,  2.37561077e-01,\n",
       "         5.62814586e-02, -8.20701271e-02,  1.46902064e-02,\n",
       "         3.01632285e-01,  1.65726230e-01,  1.88712806e-01,\n",
       "         1.00693973e-02, -1.15121536e-01, -1.22810066e-01,\n",
       "         1.25745311e-01],\n",
       "       [-1.24060139e-01, -3.23192447e-01,  9.97255743e-02,\n",
       "         1.41312465e-01,  1.70614630e-01, -1.72767356e-01,\n",
       "        -3.94757045e-03,  1.91807210e-01, -1.05596803e-01,\n",
       "        -4.10650559e-02, -4.49133515e-02,  1.56413034e-01,\n",
       "         7.10222125e-03,  9.11747664e-02, -4.03673053e-02,\n",
       "         1.71096832e-01],\n",
       "       [-6.49955496e-02, -1.32958755e-01, -1.04930997e-01,\n",
       "        -1.21657982e-01, -1.31409556e-01,  4.39380892e-02,\n",
       "         7.53188506e-02,  2.37408914e-02, -2.13189781e-01,\n",
       "        -2.25991040e-01, -1.96965560e-01,  1.91012740e-01,\n",
       "         1.32172897e-01, -1.80570930e-01,  1.66500956e-01,\n",
       "         1.65305510e-01],\n",
       "       [ 1.18393995e-01, -3.45399261e-01, -9.74905416e-02,\n",
       "        -9.79590863e-02, -1.75986826e-01,  1.53130934e-01,\n",
       "         2.22755268e-01,  1.02873690e-01,  4.31154259e-02,\n",
       "        -2.47242972e-01, -2.54048407e-01, -1.07574485e-01,\n",
       "         1.15544401e-01, -4.08093035e-02, -7.84911141e-02,\n",
       "        -4.31550890e-02],\n",
       "       [-2.55358845e-01, -2.33833805e-01,  2.31215283e-01,\n",
       "         3.10006812e-02,  9.56590753e-03, -2.38975868e-01,\n",
       "         5.54472059e-02, -4.53338213e-02,  1.11542821e-01,\n",
       "        -2.98081823e-02,  4.77156928e-03,  6.32055551e-02,\n",
       "        -1.42347723e-01,  1.55834213e-01, -1.75306126e-01,\n",
       "        -1.58368930e-01],\n",
       "       [ 2.40234032e-01,  1.48340866e-01, -1.43352568e-01,\n",
       "        -1.02147818e-01, -2.41620272e-01, -2.11238593e-01,\n",
       "         3.34037185e-01, -1.15504920e-01, -6.05773330e-02,\n",
       "         2.04522774e-01, -1.05924472e-01, -1.32869303e-01,\n",
       "         2.34175250e-01, -1.32505625e-01, -1.53554350e-01,\n",
       "         9.24210399e-02],\n",
       "       [ 3.28918308e-01,  3.97802740e-01,  2.58792847e-01,\n",
       "        -1.92637280e-01,  2.35630542e-01,  1.90675661e-01,\n",
       "         2.46757999e-01,  2.62619197e-01,  3.39490950e-01,\n",
       "         3.09332997e-01,  3.69522870e-02,  1.84251908e-02,\n",
       "         3.86086553e-01, -1.17761530e-01,  1.37263194e-01,\n",
       "        -8.37526470e-02],\n",
       "       [-8.51201341e-02,  8.53900537e-02, -6.52528107e-02,\n",
       "        -3.59279290e-02, -7.75461569e-02,  7.46999010e-02,\n",
       "        -6.92899153e-02, -2.15621218e-01, -3.33427876e-01,\n",
       "        -1.95258185e-01,  2.54444331e-01,  9.07424721e-04,\n",
       "         1.26586094e-01, -8.51966962e-02, -2.42477268e-01,\n",
       "         4.04935479e-02]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "for i in model.layers[2].get_weights():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'alpha_dropout_10/cond/Merge:0' shape=(?, 16) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].get_output_at(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sequential_1_1/alpha_dropout_10/cond/Merge:0' shape=(?, 16) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].get_output_at(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1a297c9eb8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "intermediate_layer_model = Model(inputs=[model.get_layer(index=0).get_input_at(0),\n",
    "                                         model.get_layer(index=1).get_input_at(0)]\n",
    "                                 ,outputs=[model.get_layer(index=1).get_output_at(0),\n",
    "                                           model.get_layer(index=1).get_output_at(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(data.transpose().iloc[0:5,:])\n",
    "b = np.asarray(data.transpose().iloc[6:11,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# with a Sequential model\n",
    "get_3rd_layer_output = K.function([model.layers[0].input,model.layers[1].input]\n",
    "                                  ,[model.layers[2].get_output_at(1),model.layers[2].get_output_at(2),model.layers[3].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.4162121 ,  4.810144  , -1.6980445 ,  1.1788609 ,  2.8066883 ,\n",
       "          0.80700696, -1.7158511 ,  2.5211751 ,  1.3627063 ,  3.1732688 ,\n",
       "          0.7874196 ,  0.57305425,  0.14707136,  1.3254292 ,  2.5904388 ,\n",
       "          0.47538853],\n",
       "        [-0.37818435, -0.52372587, -0.8668064 , -0.22572114, -0.09839456,\n",
       "          0.16721722,  1.0830376 , -0.9204821 ,  1.9335619 , -1.0867276 ,\n",
       "         -0.7180021 ,  0.08871941, -1.0329604 ,  0.8129696 ,  1.5617628 ,\n",
       "         -0.66331667],\n",
       "        [-0.940143  ,  4.3024282 , -1.637966  ,  0.732653  ,  2.4457934 ,\n",
       "          0.30171287, -1.7070627 ,  2.733851  ,  1.3159599 ,  2.6855285 ,\n",
       "          0.51377   ,  0.33176097,  0.01913486,  1.062967  ,  2.3257005 ,\n",
       "          0.55971825],\n",
       "        [ 0.88518065, -1.0465701 ,  1.2154307 , -1.34556   , -1.5633222 ,\n",
       "          1.0690651 ,  1.084691  , -1.4652343 ,  0.10886915, -1.2274033 ,\n",
       "         -0.72411335, -1.0061591 ,  0.7194428 , -0.5950448 , -1.0028086 ,\n",
       "         -0.26274145],\n",
       "        [-1.3458608 ,  3.5845022 , -1.5946133 ,  1.4152145 ,  2.2770784 ,\n",
       "         -0.28739855, -1.5978693 ,  2.0388768 ,  1.1827816 ,  1.4941622 ,\n",
       "          0.67395884,  0.604782  , -0.12185776,  1.2845721 ,  2.3877513 ,\n",
       "         -0.21074182]], dtype=float32),\n",
       " array([[ 0.13278784, -1.0334347 , -0.483458  , -0.88729924, -1.2695696 ,\n",
       "          1.1744351 ,  1.2933486 , -1.2672793 , -1.1176659 , -0.5699663 ,\n",
       "         -0.714495  ,  0.01719399,  0.48001304, -1.1348864 , -1.5176855 ,\n",
       "          0.04858331],\n",
       "        [-0.13410509, -0.9821513 , -0.9847327 , -1.1397507 , -1.285745  ,\n",
       "          0.7115938 ,  0.951613  , -1.326016  ,  1.2398926 , -1.5035654 ,\n",
       "         -1.055625  ,  0.37951547,  0.66536313, -0.23254554,  0.63082385,\n",
       "         -0.66033447],\n",
       "        [ 0.7425277 , -1.2842593 ,  1.1856865 , -1.0687112 , -1.4405066 ,\n",
       "          1.1183608 ,  1.2276539 , -1.5589104 , -1.6444156 ,  0.00304824,\n",
       "          0.28571466, -0.6782867 ,  0.6380733 , -0.32850382, -1.2961644 ,\n",
       "         -0.42248648],\n",
       "        [ 0.58639896, -1.3106215 ,  1.0777452 , -1.0049934 , -1.3810796 ,\n",
       "          1.1250045 ,  1.1366308 , -1.5722113 , -1.6879762 ,  0.11684655,\n",
       "          0.5354974 , -0.4546446 ,  0.7440075 , -0.11948068, -1.3207812 ,\n",
       "         -0.39648753],\n",
       "        [-1.0583271 ,  2.6070635 , -1.4041365 ,  0.64799863,  1.6892225 ,\n",
       "         -0.94579667, -1.3080544 ,  1.6753199 ,  1.4270022 ,  0.39414108,\n",
       "          0.31222817,  0.796868  , -0.6527741 ,  1.2439548 ,  2.4195273 ,\n",
       "         -0.6795729 ]], dtype=float32),\n",
       " array([[37.524235],\n",
       "        [ 9.419832],\n",
       "        [37.320236],\n",
       "        [ 7.342252],\n",
       "        [ 7.09249 ]], dtype=float32)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output = get_3rd_layer_output([a,b])\n",
    "layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = layer_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "two = layer_output[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.524234503507614"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = sum(abs(one - two))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14,\n",
       " 20,\n",
       " 9,\n",
       " 7,\n",
       " 37,\n",
       " 17,\n",
       " 28,\n",
       " 35,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 20,\n",
       " 35,\n",
       " 15,\n",
       " 8,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 13,\n",
       " 3,\n",
       " 28,\n",
       " 37,\n",
       " 19,\n",
       " 12,\n",
       " 0,\n",
       " 26,\n",
       " 6,\n",
       " 19,\n",
       " 29,\n",
       " 3,\n",
       " 10,\n",
       " 13,\n",
       " 16,\n",
       " 2,\n",
       " 0,\n",
       " 35,\n",
       " 8,\n",
       " 31,\n",
       " 17,\n",
       " 37,\n",
       " 11,\n",
       " 13,\n",
       " 7,\n",
       " 29,\n",
       " 37,\n",
       " 2,\n",
       " 10,\n",
       " 38,\n",
       " 14,\n",
       " 35,\n",
       " 33,\n",
       " 20,\n",
       " 8,\n",
       " 35,\n",
       " 12,\n",
       " 30,\n",
       " 4,\n",
       " 22,\n",
       " 25,\n",
       " 23,\n",
       " 33,\n",
       " 22,\n",
       " 11,\n",
       " 24,\n",
       " 35,\n",
       " 4,\n",
       " 30,\n",
       " 14,\n",
       " 38,\n",
       " 22,\n",
       " 29,\n",
       " 33,\n",
       " 9,\n",
       " 32,\n",
       " 7,\n",
       " 34,\n",
       " 35,\n",
       " 26,\n",
       " 18,\n",
       " 29,\n",
       " 26,\n",
       " 24,\n",
       " 5,\n",
       " 24,\n",
       " 37,\n",
       " 2,\n",
       " 8,\n",
       " 31,\n",
       " 38,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 3,\n",
       " 8,\n",
       " 17,\n",
       " 30,\n",
       " 27,\n",
       " 34,\n",
       " 18,\n",
       " 20,\n",
       " 9,\n",
       " 18,\n",
       " 31,\n",
       " 30,\n",
       " 33,\n",
       " 26,\n",
       " 27,\n",
       " 8,\n",
       " 23,\n",
       " 28,\n",
       " 22,\n",
       " 1,\n",
       " 17,\n",
       " 5,\n",
       " 23,\n",
       " 18,\n",
       " 19,\n",
       " 2,\n",
       " 29,\n",
       " 19,\n",
       " 16,\n",
       " 39,\n",
       " 30,\n",
       " 23,\n",
       " 14,\n",
       " 25,\n",
       " 8,\n",
       " 29,\n",
       " 25,\n",
       " 11,\n",
       " 3,\n",
       " 1,\n",
       " 31,\n",
       " 35,\n",
       " 39,\n",
       " 24,\n",
       " 27,\n",
       " 16,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 11,\n",
       " 28,\n",
       " 33,\n",
       " 23,\n",
       " 22,\n",
       " 33,\n",
       " 3,\n",
       " 36,\n",
       " 11,\n",
       " 1,\n",
       " 0,\n",
       " 25,\n",
       " 27,\n",
       " 21,\n",
       " 35,\n",
       " 32,\n",
       " 33,\n",
       " 15,\n",
       " 7,\n",
       " 11,\n",
       " 32,\n",
       " 29,\n",
       " 9,\n",
       " 12,\n",
       " 32,\n",
       " 16,\n",
       " 6,\n",
       " 23,\n",
       " 31,\n",
       " 2,\n",
       " 15,\n",
       " 16,\n",
       " 31,\n",
       " 16,\n",
       " 32,\n",
       " 34,\n",
       " 25,\n",
       " 25,\n",
       " 21,\n",
       " 9,\n",
       " 20,\n",
       " 8,\n",
       " 10,\n",
       " 37,\n",
       " 18,\n",
       " 1,\n",
       " 1,\n",
       " 32,\n",
       " 6,\n",
       " 3,\n",
       " 10,\n",
       " 18,\n",
       " 10,\n",
       " 6,\n",
       " 37,\n",
       " 38,\n",
       " 27,\n",
       " 1,\n",
       " 5,\n",
       " 24,\n",
       " 33,\n",
       " 34,\n",
       " 36,\n",
       " 38,\n",
       " 27,\n",
       " 24,\n",
       " 19,\n",
       " 9,\n",
       " 12,\n",
       " 39,\n",
       " 21,\n",
       " 20,\n",
       " 25,\n",
       " 3,\n",
       " 26,\n",
       " 33,\n",
       " 3,\n",
       " 9,\n",
       " 38,\n",
       " 3,\n",
       " 21,\n",
       " 38,\n",
       " 35,\n",
       " 10,\n",
       " 13,\n",
       " 14,\n",
       " 11,\n",
       " 21,\n",
       " 25,\n",
       " 16,\n",
       " 0,\n",
       " 36,\n",
       " 38,\n",
       " 28,\n",
       " 31,\n",
       " 13,\n",
       " 35,\n",
       " 28,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 14,\n",
       " 26,\n",
       " 15,\n",
       " 18,\n",
       " 11,\n",
       " 10,\n",
       " 3,\n",
       " 24,\n",
       " 22,\n",
       " 24,\n",
       " 38,\n",
       " 8,\n",
       " 5,\n",
       " 28,\n",
       " 17,\n",
       " 12,\n",
       " 12,\n",
       " 4,\n",
       " 38,\n",
       " 5,\n",
       " 38,\n",
       " 27,\n",
       " 26,\n",
       " 12,\n",
       " 12,\n",
       " 29,\n",
       " 15,\n",
       " 18,\n",
       " 3,\n",
       " 34,\n",
       " 37,\n",
       " 1,\n",
       " 36,\n",
       " 12,\n",
       " 38,\n",
       " 33,\n",
       " 23,\n",
       " 24,\n",
       " 17,\n",
       " 4,\n",
       " 36,\n",
       " 1,\n",
       " 35,\n",
       " 10,\n",
       " 11,\n",
       " 6,\n",
       " 4,\n",
       " 39,\n",
       " 12,\n",
       " 10,\n",
       " 31,\n",
       " 8,\n",
       " 28,\n",
       " 4,\n",
       " 34,\n",
       " 36,\n",
       " 32,\n",
       " 12,\n",
       " 11,\n",
       " 0,\n",
       " 4,\n",
       " 15,\n",
       " 39,\n",
       " 1,\n",
       " 22,\n",
       " 2,\n",
       " 32,\n",
       " 27,\n",
       " 2,\n",
       " 1,\n",
       " 35,\n",
       " 2,\n",
       " 4,\n",
       " 23,\n",
       " 34,\n",
       " 31,\n",
       " 23,\n",
       " 34,\n",
       " 13,\n",
       " 21,\n",
       " 9,\n",
       " 32,\n",
       " 19,\n",
       " 23,\n",
       " 15,\n",
       " 2,\n",
       " 10,\n",
       " 27,\n",
       " 6,\n",
       " 38,\n",
       " 9,\n",
       " 36,\n",
       " 31,\n",
       " 16,\n",
       " 28,\n",
       " 38,\n",
       " 28,\n",
       " 10,\n",
       " 26,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 17,\n",
       " 7,\n",
       " 7,\n",
       " 19,\n",
       " 18,\n",
       " 1,\n",
       " 3,\n",
       " 26,\n",
       " 21,\n",
       " 7,\n",
       " 31,\n",
       " 2,\n",
       " 25,\n",
       " 18,\n",
       " 32,\n",
       " 5,\n",
       " 3,\n",
       " 36,\n",
       " 33,\n",
       " 5,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 19,\n",
       " 33,\n",
       " 10,\n",
       " 16,\n",
       " 29,\n",
       " 21,\n",
       " 18,\n",
       " 12,\n",
       " 18,\n",
       " 33,\n",
       " 19,\n",
       " 19,\n",
       " 3,\n",
       " 35,\n",
       " 28,\n",
       " 12,\n",
       " 15,\n",
       " 18,\n",
       " 7,\n",
       " 13,\n",
       " 26,\n",
       " 19,\n",
       " 6,\n",
       " 14,\n",
       " 19,\n",
       " 35,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 36,\n",
       " 32,\n",
       " 17,\n",
       " 34,\n",
       " 29,\n",
       " 37,\n",
       " 12,\n",
       " 8,\n",
       " 31,\n",
       " 31,\n",
       " 26,\n",
       " 32,\n",
       " 34,\n",
       " 16,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 18,\n",
       " 33,\n",
       " 13,\n",
       " 9,\n",
       " 20,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 34,\n",
       " 36,\n",
       " 20,\n",
       " 1,\n",
       " 0,\n",
       " 25,\n",
       " 22,\n",
       " 34,\n",
       " 17,\n",
       " 37,\n",
       " 25,\n",
       " 25,\n",
       " 30,\n",
       " 16,\n",
       " 12,\n",
       " 37,\n",
       " 9,\n",
       " 32,\n",
       " 18,\n",
       " 35,\n",
       " 2,\n",
       " 0,\n",
       " 15,\n",
       " 21,\n",
       " 9,\n",
       " 28,\n",
       " 10,\n",
       " 30,\n",
       " 15,\n",
       " 37,\n",
       " 32,\n",
       " 13,\n",
       " 15,\n",
       " 28,\n",
       " 39,\n",
       " 1,\n",
       " 12,\n",
       " 31,\n",
       " 20,\n",
       " 13,\n",
       " 17,\n",
       " 18,\n",
       " 12,\n",
       " 21,\n",
       " 32,\n",
       " 37,\n",
       " 36,\n",
       " 3,\n",
       " 34,\n",
       " 39,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 19,\n",
       " 7,\n",
       " 25,\n",
       " 5,\n",
       " 24,\n",
       " 14,\n",
       " 7,\n",
       " 19,\n",
       " 33,\n",
       " 39,\n",
       " 27,\n",
       " 17,\n",
       " 16,\n",
       " 4,\n",
       " 35,\n",
       " 13,\n",
       " 20,\n",
       " 30,\n",
       " 4,\n",
       " 13,\n",
       " 30,\n",
       " 17]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
